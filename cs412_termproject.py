# -*- coding: utf-8 -*-
"""CS412-TermProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QJdNfZzHf2RnkZtCx-XTPv8sBQm3MK7y

## Introduction

This CS 412 course project aims to predict the grades of the students from their ChatGPT histories. To compare the test results, many models are perfomed and evaluated. Both the traiditonal machine learning (with scikit-learn) and deep learning (with TensorFlow and Keras) are applied in this project so that a more comprehensive results can be reached.

## Import Libraries
"""

# import the necessary libraries
import os
import re
import tqdm
import nltk
import seaborn as sns
from glob import glob
from pathlib import Path
import logging

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from pprint import pprint
import graphviz

from collections import defaultdict
from bs4 import BeautifulSoup

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from scipy import sparse

from sklearn import tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from os.path import join
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow.keras.backend as K
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk import pos_tag, word_tokenize
from nltk.sentiment import SentimentIntensityAnalyzer

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""## 1. Data Preparation"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Specify the directory path for HTML files
html_files_path = '/content/drive/My Drive/CS412/dataset/*.html'

"""### 1.1 **Extract conversations from HTML files**"""

# Set up logging configuration
logging.basicConfig(filename='error_log.txt', level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')

def extract_conversations(html_path):
    """
    Extract conversations from an HTML file.
    Returns a list of dictionaries with role and text.
    """
    try:
        with open(html_path, "r", encoding="latin1") as fh:
            html_page = fh.read()

        soup = BeautifulSoup(html_page, "html.parser")
        data_test_id_pattern = re.compile(r"conversation-turn-[0-9]+")
        conversations = soup.find_all("div", attrs={"data-testid": data_test_id_pattern})

        convo_texts = []
        for convo in conversations:
            convo = convo.find_all("div", attrs={"data-message-author-role": re.compile(r"[user|assistant]")})
            if len(convo) > 0:
                role = convo[0].get("data-message-author-role")
                convo_texts.append({
                    "role": role,
                    "text": convo[0].text
                })

        return convo_texts
    except Exception as e:
        # Log the error
        logging.error(f"Error processing {html_path}: {e}")
        # Re-raise the exception to stop the execution (optional)
        raise

code2convos = dict()
pbar = tqdm.tqdm(sorted(list(glob(html_files_path))))

for path in pbar:
    file_code = os.path.basename(path).split(".")[0]
    try:
        code2convos[file_code] = extract_conversations(path)
    except Exception as e:
        print(f"Error processing {path}: {e}")

"""A brief explanation for this piece of code:

1) "extract_conversations" function reads an HTML file, parses it with BeautifulSoup, and extracts conversations based on specified patterns.

2) "for path in pbar" iterates through HTML files using a progress bar (tqdm).

3) "file_code" is extracted from the file path to serve as a key in the "code2convos" dictionary.

4) Inside the loop, "extract_conversations" function is called for each HTML file, and the results are stored in the dictionary.

Note: Regarding "exception handling (try/except)", the "logging.basicConfig" sets up basic configuration for logging. It writes errors to a file named "error_log.txt".

This way, when an exception occurs, log file captures details about the errors, which can be helpful for debugging and analysis.
"""

# verifies whether the html_files_path correctly reaches the dataset
print(sorted(list(glob(html_files_path))))

# confirms whether the conversation extraction and dictionary creation process worked as expected
pprint(list(code2convos.keys()))

# checks if keys with additional "(1)" in the name are duplicated

file_codes_to_compare = ['a1e834df-f4f6-4962-bcda-17f8aefc7f86 (1)', 'a1e834df-f4f6-4962-bcda-17f8aefc7f86']

# Check if both file codes are present in code2convos
if all(file_code in code2convos for file_code in file_codes_to_compare):
    # Extract conversations for each file code
    convos1 = code2convos[file_codes_to_compare[0]]
    convos2 = code2convos[file_codes_to_compare[1]]

    # Check if conversations are exactly the same
    are_conversations_equal = convos1 == convos2

    if are_conversations_equal:
        print("Conversations for both file codes are exactly the same.")
    else:
        print("Conversations for both file codes are different.")
else:
    print("One or both file codes not found in code2convos.")

"""A brief explanation for this piece of code:

This part checks whether both file codes (one with an additional "(1)") are present in the "code2convos" dictionary. If both file codes are present, it extracts conversations for each file code. Then, it checks whether the conversations for both file codes are exactly the same.
"""

# List of file codes to drop
file_codes_to_drop = ['6a2003ad-a05a-41c9-9d48-e98491a90499 (1)', 'a1e834df-f4f6-4962-bcda-17f8aefc7f86 (1)']

# Identify and drop duplicates from code2convos
code2convos_no_duplicates = {file_code: convos for file_code, convos in code2convos.items()
                              if file_code not in file_codes_to_drop}

# Print information about the dropped duplicates
dropped_duplicates = set(code2convos.keys()) - set(code2convos_no_duplicates.keys())
print(f"Dropped {len(dropped_duplicates)} duplicates based on file codes.")

# Update code2convos with the deduplicated dictionary
code2convos = code2convos_no_duplicates

"""A brief explanation for this piece of code:

This part identifies and drops duplicates from the "code2convos" dictionary based on "file_codes_to_drop".

The code2convos dictionary is updated with the deduplicated version
"""

# checks one of the conversations
pprint(code2convos["da219169-aacb-48b8-abdc-e25f08ad029e"][0])

"""### 1.2 **Preprocessing text in conversations**"""

# Download NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

def preprocess_text(text):
    # Lowercasing
    text = text.lower()

    # Removing Punctuation and Special Characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Tokenization
    tokens = word_tokenize(text)

    # Removing Stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Join the tokens back into a single string
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text

# Apply preprocessing to the text in code2convos
preprocessed_code2convos = {}

for file_code, convos in code2convos.items():
    preprocessed_convos = []
    for convo in convos:
        preprocessed_text = preprocess_text(convo['text'])
        preprocessed_convos.append({'role': convo['role'], 'text': preprocessed_text})
    preprocessed_code2convos[file_code] = preprocessed_convos

"""A bried explanation for this piece of code:

"preprocess_text" function takes a text as an input and performs the following preprocessing steps:

1) lowercasing the text

2) removing punctuation and special characters

3) tokenizing the text

4) removing stopwords

5) lemmatizing the tokens

6) joining the tokens back into a single string

"""

# Print one preprocessed prompt for a specific file code
file_code_to_inspect = "da219169-aacb-48b8-abdc-e25f08ad029e"
preprocessed_prompts = [conv['text'] for conv in preprocessed_code2convos[file_code_to_inspect] if conv['role'] == 'user'] #or use 'assistant' for chatgpt response

if preprocessed_prompts:
    print(f"Original Prompts for file {file_code_to_inspect}:")
    pprint([conv['text'] for conv in code2convos[file_code_to_inspect] if conv['role'] == 'user'])

    print(f"\nPreprocessed Prompts for file {file_code_to_inspect}:")
    pprint(preprocessed_prompts)
else:
    print(f"No user prompts found for file {file_code_to_inspect}.")

"""### 1.3 **Prompts matching with questions**
> We want to match the prompts with the questions in the Homework Let's
> do it with a simple term frequency vectorizing method. For each prompt,
> we will come with a vector that represents it. We will do the same
> thing with each of the homework questions. Then, we will calculate the
> vectors distance to do the matching
"""

prompts = []
code2prompts = defaultdict(list)

for code, convos in preprocessed_code2convos.items():
    user_prompts = [conv["text"] for conv in convos if conv["role"] == "user"]
    prompts.extend(user_prompts)
    code2prompts[code] = user_prompts

# Process prompts
prompts = [preprocess_text(prompt) for prompt in prompts]

print(prompts[0])

questions = [
    """Initialize
*   First make a copy of the notebook given to you as a starter.
*   Make sure you choose Connect form upper right.
*   You may upload the data to the section on your left on Colab, than right click on the .csv file and get the path of the file by clicking on "Copy Path". You will be using it when loading the data.

""",
#####################
    """Load training dataset (5 pts)
    *  Read the .csv file with the pandas library
""",
#####################
"""Understanding the dataset & Preprocessing (15 pts)
Understanding the Dataset: (5 pts)
> - Find the shape of the dataset (number of samples & number of attributes). (Hint: You can use the **shape** function)
> - Display variable names (both dependent and independent).
> - Display the summary of the dataset. (Hint: You can use the **info** function)
> - Display the first 5 rows from training dataset. (Hint: You can use the **head** function)
Preprocessing: (10 pts)

> - Check if there are any missing values in the dataset. If there are, you can either drop these values or fill it with most common values in corresponding rows. **Be careful that you have enough data for training the  model.**

> - Encode categorical labels with the mappings given in the cell below. (Hint: You can use **map** function)
""",
"""Set X & y, split data (5 pts)

*   Shuffle the dataset.
*   Seperate your dependent variable X, and your independent variable y. The column health_metrics is y, the rest is X.
*   Split training and test sets as 80% and 20%, respectively.
""",
#####################
"""Features and Correlations (10 pts)

* Correlations of features with health (4 points)
Calculate the correlations for all features in dataset. Highlight any strong correlations with the target variable. Plot your results in a heatmap.

* Feature Selection (3 points)
Select a subset of features that are likely strong predictors, justifying your choices based on the computed correlations.

* Hypothetical Driver Features (3 points)
Propose two hypothetical features that could enhance the model's predictive accuracy for Y, explaining how they might be derived and their expected impact. Show the resulting correlations with target variable.

* __Note:__ You get can get help from GPT.
""",
#####################
"""Tune Hyperparameters (20 pts)
* Choose 2 hyperparameters to tune. You can use the Scikit learn decision tree documentation for the available hyperparameters *(Hyperparameters are listed under "Parameters" in the documentation)*. Use GridSearchCV for hyperparameter tuning, with a cross-validation value of 5. Use validation accuracy to pick the best hyper-parameter values. (15 pts)
-Explain the hyperparameters you chose to tune. *(What are the hyperparameters you chose? Why did you choose them?)* (5 pts)
""",
#####################
"""Re-train and plot the decision tree with the hyperparameters you have chosen (15 pts)
- Re-train model with the hyperparameters you have chosen in part 5). (10 pts)
- Plot the tree you have trained. (5 pts)
Hint: You can import the **plot_tree** function from the sklearn library.
""",
#####################
"""Test your classifier on the test set (20 pts)
- Predict the labels of testing data using the tree you have trained in step 6. (10 pts)
- Report the classification accuracy. (2 pts)
- Plot & investigate the confusion matrix. Fill the following blanks. (8 pts)
> The model most frequently mistakes class(es) _________ for class(es) _________.
Hint: You can use the confusion_matrix function from sklearn.metrics
""",
#####################
"""Find the information gain on the first split (10 pts)""",
#####################
]

# Process homework questions
questions = [preprocess_text(question) for question in questions]

# Concatenate prompts and questions for fitting the vectorizer
all_texts = prompts + questions

# Fit the vectorizer on all texts
vectorizer = TfidfVectorizer().fit(all_texts)

# Transform prompts and questions using the fitted vectorizer
prompts_TF_IDF = vectorizer.transform(prompts)
questions_TF_IDF = vectorizer.transform(questions)

# Create DataFrames
prompts_df = pd.DataFrame(prompts_TF_IDF.toarray(), columns=vectorizer.get_feature_names_out())
questions_df = pd.DataFrame(questions_TF_IDF.toarray(), columns=vectorizer.get_feature_names_out())

# Dictionary to store TF-IDF representations
code2prompts_tf_idf = {}

# Transform user prompts and store in the dictionary
for code, user_prompts in code2prompts.items():
    if len(user_prompts) == 0:
        # some files have issues
        print(code + ".html")
        continue

    prompts_TF_IDF = vectorizer.transform(user_prompts)
    code2prompts_tf_idf[code] = pd.DataFrame(prompts_TF_IDF.toarray(), columns=vectorizer.get_feature_names_out())

# Convert DataFrames to sparse matrices
questions_sparse = sparse.csr_matrix(questions_df.values)

code2cosine = dict()
for code, user_prompts_tf_idf in code2prompts_tf_idf.items():
    # Ensure that the dimensions are aligned for cosine similarity
    user_prompts_sparse = sparse.csr_matrix(user_prompts_tf_idf.values)  # Use .values to get the underlying array

    # Ensure both matrices have the same number of columns
    if questions_sparse.shape[1] != user_prompts_sparse.shape[1]:
        # Skip cases with dimension mismatch
        print(f"Warning: Dimension mismatch for {code}. Skipping...")
        continue

    # Calculate cosine similarity
    code2cosine[code] = cosine_similarity(questions_sparse, user_prompts_sparse)

"""A brief explanation for this piece of code:

1) Fit the TF-IDF vectorizer

2) Transform prompts and questions

3) Create DataFrames ("prompts_df" and "questions_df")

4) For each file code, transform user prompts into TF-IDF representations and store them in "code2prompts_tf_idf"

5) Calculate cosine similary and store the results in "code2cosine"
"""

# Check the TF-IDF representation for a specific file code
file_code_to_check = "089eb66d-4c3a-4f58-b98f-a3774a2efb34"

# Access the TF-IDF DataFrame for the specified file code
tf_idf_for_file = code2prompts_tf_idf[file_code_to_check]

# Print the first few rows of the DataFrame
print(tf_idf_for_file.head())

# Check the content of the fifth question after preprocessing
question_to_check = questions[4]

# Print the content of the question
print(question_to_check)

"""### 1.4 **Tokenization and vectorization**
> The tokenization and vectorization part is necessary since we are planning to use neural networks, as neural networks require input data to be in a numerical format. Tokenization is the process of converting text into a sequence of integers, and it's a common preprocessing step for natural language processing tasks.
"""

# Combine prompts and questions
all_text = prompts + questions

# Tokenize text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_text)

# Convert text to sequences of integers
prompts_sequences = tokenizer.texts_to_sequences(prompts)
questions_sequences = tokenizer.texts_to_sequences(questions)

# Pad sequences for uniform length
max_sequence_length = max(max(len(seq) for seq in prompts_sequences), max(len(seq) for seq in questions_sequences))
prompts_padded = pad_sequences(prompts_sequences, maxlen=max_sequence_length, padding='post')
questions_padded = pad_sequences(questions_sequences, maxlen=max_sequence_length, padding='post')

code2questionmapping = dict()
for code, cosine_scores in code2cosine.items():
    code2questionmapping[code] = code2cosine[code].max(axis=1).tolist()

question_mapping_scores = pd.DataFrame(code2questionmapping).T
question_mapping_scores.reset_index(inplace=True)
question_mapping_scores.rename(columns={i: f"Q_{i}" for i in range(len(questions))}, inplace=True)
question_mapping_scores.rename(columns={"index" : "code"}, inplace=True)
question_mapping_scores

"""A brief explanation for this piece of code:

 This part creates a mapping between file codes and their corresponding cosine similarity scores for each question.

 This DataFrame provides a summary of the maximum cosine similarity scores between each file code and each question.
"""

# reading the scores
filename = "scores.csv"
path_prefix = './drive/My Drive/CS412/'
scores = pd.read_csv(join(path_prefix, filename))

scores["code"] = scores["code"].apply(lambda x: x.strip())

# selecting the columns we need and we care
scores = scores[["code", "grade"]]

scores

"""# Feature Engineering

These are the features that are provided by the base code:
- Feature 1: Number of prompts that a users asked
- Feature 2: Number of complaints that a user makes e.g "the code gives this error!"
- Feature 3: User prompts average number of characters
"""

code2features = defaultdict(lambda: defaultdict(int))

keywords_to_search = ["error", "no", "thank", "next", "Entropy"]
keywords_to_search = [kw.lower() for kw in keywords_to_search]

for code, convs in preprocessed_code2convos.items():
    if len(convs) == 0:
        print(code)
        continue

    for c in convs:
        text = c["text"].lower()

        if c["role"] == "user":
            # User Prompts Count
            code2features[code]["#user_prompts"] += 1

            # Keyword Counts (#error, #no, #thank, #next, #entropy)
            for kw in keywords_to_search:
                code2features[code][f"#{kw}"] += text.count(kw)

            # Prompt Average Characters
            code2features[code]["prompt_avg_chars"] += len(text)
        else:
            # ChatGPT Response Average Characters
            code2features[code]["response_avg_chars"] += len(text)

# Move averaging calculations outside of the outer loop
for code in code2features:
    if code2features[code]["#user_prompts"] > 0:
        code2features[code]["prompt_avg_chars"] /= code2features[code]["#user_prompts"]
        code2features[code]["response_avg_chars"] /= code2features[code]["#user_prompts"]

"""A brief explanation for this piece of code:

 - Iterate through preprocessed conversations (convs) for each file code (code).

 - For each conversation (c), check if the role is "user" or not.

 - For user prompts, update features like the count of user prompts, counts of specific keywords, and the total number of characters in user prompts.

 - For ChatGPT responses, update the total number of characters in responses.

 - After processing all conversations for each file code, move outside the outer loop to calculate average characters for both user prompts and ChatGPT responses.
"""

nltk.download('vader_lexicon')

df = pd.DataFrame(code2features).T
df.head(5)

"""#### The features that are provided by our team:

- Feature 4: Total number of prompts

- Feature 5: Ratio of error prompts to total prompts

- Feature 6: Average entropy per prompt

- Feature 7: Total characters per interaction (sum of prompt and response averages)

- Feature 8: Ratio of prompt characters to response characters

- Feature 9: Average Q per responses

- Feature 10: Positive responses to negative responses ratio

- Feature 11: Response Complexity

- Feature 12: Response diversity

- Feature 13: Prompts to errors ratio

- Feature 14: Frequency of "thank you"

- Feature 15: Average entropy of responses

- Feature 16: Q_0 - Q_8 ratio to total prompts

- Feature 17: Response length

- Feature 18: Sentiment Analysis on Responses

- Feature 19: Frequency of repeating prompts

- Feature 20: Flesch-Kincaid readability score

- Feature 21: Question number where student starts consulting gpt

- Feature 22: Primitive Grade (calculated based on simitarity score and max point of each question)

Not all these features are used in the predictor model. They are just potential features at this step. Their correlations with the students' grades will be evaluated, and then we will decide which ones to use. Also, using this many feature is likely to cause overfit, thus they should be definetely reduced.
"""

# Each point of the corresponding question
def Q_num_to_pt(Q_num):

  if Q_num == 0:
    pt = 0
  elif Q_num == 1:
    pt = 5
  elif Q_num == 2:
    pt = 15
  elif Q_num == 3:
    pt = 5
  elif Q_num == 4:
    pt = 10
  elif Q_num == 5:
    pt = 20
  elif Q_num == 6:
    pt = 15
  elif Q_num == 7:
    pt = 20
  elif Q_num == 8:
    pt = 10
  else:
    pt = 0

  return pt

# Feature 21: Question number where student starts consulting ChatGPT
code2afterWhichQuestStartAskGPT = defaultdict(lambda : defaultdict(int))

for code, col in code2questionmapping.items():
  check = 0

  for i in range(len(questions)):
    threshold =0.46
    #print(code2afterWhichQuestStartAskGPT[code]["whichQuestStartAskGPT"])
    if ((check == 0) and (col[i]>threshold)):
      code2afterWhichQuestStartAskGPT[code]["whichQuestStartAskGPT"] = i
      check +=1
    elif ((check == 0) and (i == (len(questions)-1))):
      code2afterWhichQuestStartAskGPT[code]["whichQuestStartAskGPT"] = -1


whichQuestStartAskGPT = pd.DataFrame(code2afterWhichQuestStartAskGPT).T

whichQuestStartAskGPT.reset_index(inplace=True)
whichQuestStartAskGPT.rename(columns={"index" : "code"}, inplace=True)

whichQuestStartAskGPT

# Feature 22: Primitive Grade (calculated based on simitarity score and max point of each question)

# We assumed that students received full marks from the question they started consulting ChatGPT with and the questions that preceded it.
def total_pt_beforeGPT_assumeAllCorrect(i_questStartAskGPT):
  tot_pt = 0
  for q_num in range(0,(i_questStartAskGPT + 1)):
    tot_pt += Q_num_to_pt(q_num)
  return tot_pt

code2primitiveGrade = defaultdict(lambda : defaultdict(int))

for code, col in code2questionmapping.items():
  code2primitiveGrade[code]["primitive_grade"] = 0

  for i in range(len(questions)):
    threshold =0.46

    if((code2afterWhichQuestStartAskGPT[code]["whichQuestStartAskGPT"]) == i):
      code2primitiveGrade[code]["primitive_grade"] = (total_pt_beforeGPT_assumeAllCorrect(i))

    elif(i > (code2afterWhichQuestStartAskGPT[code]["whichQuestStartAskGPT"])):
      if (col[i] >= threshold):
        code2primitiveGrade[code]["primitive_grade"] += Q_num_to_pt(i)
      else:
        code2primitiveGrade[code]["primitive_grade"] += (col[i] *  Q_num_to_pt(i))

    elif((code2afterWhichQuestStartAskGPT[code]["whichQuestStartAskGPT"]) == -1):
      code2primitiveGrade[code]["primitive_grade"] = scores["grade"].median()

if (code2primitiveGrade[code]["primitive_grade"] < (scores['grade'].mean())):
  code2primitiveGrade[code]["primitive_grade"] = scores['grade'].mean()


primitive_grade_df = pd.DataFrame(code2primitiveGrade).T
#primitive_grade_df.info()

primitive_grade_df.reset_index(inplace=True)
primitive_grade_df.rename(columns={"index" : "code"}, inplace=True)

primitive_grade_df

!pip install textstat
from textstat import flesch_kincaid_grade

# Feature 4: Total number of prompts
df['total_prompts'] = df['#user_prompts'] + df['#error'] + df['#no'] + df['#thank'] + df['#next']

# Feature 5: Ratio of error prompts to total prompts
df['error_ratio'] = df['#error'] / df['total_prompts']

# Feature 6: Average entropy per prompt
df['avg_entropy_per_prompt'] = df['#entropy'] / df['total_prompts']

# Feature 7: Total characters per interaction (sum of prompt and response averages)
df['total_chars_per_interaction'] = df['prompt_avg_chars'] + df['response_avg_chars']

# Feature 8: Ratio of prompt characters to response characters
df['prompt_response_char_ratio'] = df['prompt_avg_chars'] / df['response_avg_chars']

# Feature 9: Average Q per responses
#df['avg_Q_responses'] = question_mapping_scores[['Q_0', 'Q_1', 'Q_2', 'Q_3', 'Q_4', 'Q_5', 'Q_6', 'Q_7', 'Q_8']].mean(axis=1)

# Feature 10: Positive responses to negative responses ratio
df['pos_to_neg_ratio'] = (df['#thank'] + df['#next']) / (df['#error'] + df['#no'] + 1)  # Adding 1 to avoid division by zero

# Feature 11: Response Complexity
df['response_complexity'] = df['response_avg_chars'] * df['#entropy']

# Feature 12: Response diversity
#df['unique_responses'] = df['response_avg_chars'].apply(lambda x: len(set(str(x).split())))

# Feature 13: Prompts to errors ratio
df['prompt_to_error_ratio'] = df['#user_prompts'] / (df['#error'] + 1)  # Adding 1 to avoid division by zero

# Feature 14: Frequency of "thank you"
df['thank_you_frequency'] = df['#thank'] / (df['#user_prompts'] + 1)  # Adding 1 to avoid division by zero

# Feature 15: Average entropy of responses
df['average_response_entropy'] = df['#entropy'] / (df['#user_prompts'] + 1)  # Adding 1 to avoid division by zero

# Feature 16: Q_0 - Q_8 ratio to total prompts
#for i in range(9):
#    df[f'Q_{i}_ratio'] = question_mapping_scores[f'Q_{i}'] / df['total_prompts']

# Feature 17: Response length
df['response_length'] = df['response_avg_chars'].astype(str).apply(len)

# Feature 18: Sentiment Analysis on Responses
sentiment_analyzer = SentimentIntensityAnalyzer()
df['response_sentiment_score'] = df['response_avg_chars'].apply(lambda x: sentiment_analyzer.polarity_scores(str(x))['compound'])

# Feature 19: Frequency of repeating prompts
df['repeating_prompt_frequency'] = df['#user_prompts'].apply(lambda x: 1 if x > 1 else 0)

# Feature 20: Flesch-Kincaid readability score
#df['readability_score'] = df['response_avg_chars'].astype(str).apply(lambda x: flesch_kincaid_grade(x))

# Rename the index to code
df.reset_index(inplace=True, drop=False)
df.rename(columns={"index": "code"}, inplace=True)
df.head()

# Feature 21: Question number where student starts consulting ChatGPT
df = pd.merge(df, whichQuestStartAskGPT, on="code", how="left")

# Feature 22: Primitive Grade (calculated based on simitarity score and max point of each question)
df = pd.merge(df, primitive_grade_df, on="code", how="left")

# Display the updated DataFrame
print(df.head())

"""Embedding Features:

from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np

# Load pre-trained BERT tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

# Assuming 'response_avg_chars' is the column containing average response characters
responses = df['response_avg_chars'].astype(str).tolist()

# Tokenize and obtain embeddings for each response
embeddings = []

for response in responses:
    tokens = tokenizer(response, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        output = model(**tokens)
    # Use the [CLS] token embedding as the representation of the entire response
    cls_embedding = output.last_hidden_state[:, 0, :].numpy()
    embeddings.append(cls_embedding)

# Concatenate the individual response embeddings into a single 2D array
embeddings_array = np.concatenate(embeddings, axis=0)

# Create column names based on the embedding dimensions
column_names = [f'response_embedding_dim_{i+1}' for i in range(embeddings_array.shape[1])]

# Create a DataFrame with the embeddings
embeddings_df = pd.DataFrame(embeddings_array, columns=column_names)

# Concatenate the original DataFrame with the new embeddings DataFrame
df = pd.concat([df, embeddings_df], axis=1)

A brief explanation for this piece of code:

1) Tokenize each response, generate BERT embeddings, and extract the embedding of the [CLS] token.

2) Concatenate individual response embeddings into a single 2D array.

3) Create a DataFrame with columns named 'response_embedding_dim_1', 'response_embedding_dim_2', etc.

4) Concatenate the original DataFrame with the new embeddings DataFrame.

embedding_columns = [f'response_embedding_dim_{i+1}' for i in range(embeddings_array.shape[1])]

# Feature 17: Mean of response embeddings
df['response_embedding_mean'] = df[embedding_columns].mean(axis=1)

# Feature 18: Standard deviation of response embeddings
df['response_embedding_std'] = df[embedding_columns].std(axis=1)

# Feature 19: Minimum of response embeddings
df['response_embedding_min'] = df[embedding_columns].min(axis=1)

# Feature 20: Maximum of response embeddings
df['response_embedding_max'] = df[embedding_columns].max(axis=1)

# Display the updated DataFrame
print(df.head())

A brief explanation for this piece of code:

Computes mean, standard deviation, minimum, and maximum of the response embeddings and adds them as new features.

NLP-based Feature:
"""

# Download NLTK resources
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

def ner_extraction(text):
    # Ensure that the text is a string
    text = str(text)

    # Tokenize the text
    words = word_tokenize(text)

    # Perform POS tagging
    pos_tags = pos_tag(words)

    # Extract NER tags
    ner_tags = [tag for _, tag in pos_tags]

    return ner_tags

# Initialize NER counts for preprocessed data
pp_ner_counts = defaultdict(int)

# Iterate through the preprocessed_code2convos dictionary
for code, convos in preprocessed_code2convos.items():
    responses = [conv["text"] for conv in convos if conv["role"] == "assistant"]
    ner_counts = [len(ner_extraction(response)) for response in responses]
    pp_ner_counts[code] = sum(ner_counts)

# Apply NER extraction to the 'response_avg_chars' column and create a new column 'ner_count'
df['ner_count'] = df['response_avg_chars'].apply(lambda x: len(ner_extraction(x)))

"""A brief explanation for this piece of code:

1) Tokenize the input text, perform Part-of-Speech tagging, and extract Named Entity Recognition tags.

2) Iterate through the 'preprocessed_code2convos' dictionary, extract responses, apply the NER function to each response, and count the total number of Named Entities for each code.

3) Apply the NER function to the 'response_avg_chars' column in the DataFrame, creating a new column 'ner_count' that represents the count of Named Entities in each response.


"""

df

# Let's check grades distribution

plt.title('Histogram Grades')
plt.hist(scores["grade"], rwidth=.8, bins=np.arange(min(scores["grade"]), max(scores["grade"])+2) - 0.5)
plt.ylabel('Count')
plt.show()

"""Histogram Plot of Grades:

- The peak is very close to 100, indicating that most of the students get a very high grade. Also, there seems to be only a few low grades.

- The grades generally varies between 80 to 100.

- This is a left-skewed histogram with a tail on the left, which implies that fewer students receiverd lower grades.

- Outliers are the lowest grades. There is a 15 and a 31 in tha dataset.

- This is an asymmetric histogram. Generally the "normal ditribution" is very common in the nature, but this histogram do not have a bell-shaped curve. It is skewed.

DataFrame now has a new index, and the column previously named "index" is now named "code."
"""

nan_columns = df.columns[df.isna().any()].tolist()
print("Columns with NaN values:", nan_columns)

"""This piece of code checks for columns in the DataFrame (df) that contain NaN (Not a Number) values and prints those columns. NaN values often indicate missing or undefined data in a DataFrame.

Merging scores with features
"""

df = pd.merge(df, scores, on='code', how="left")
df.dropna(inplace=True)
df.drop_duplicates("code",inplace=True, keep="first")
df.head()

df.columns

from sklearn.preprocessing import MinMaxScaler
#Scaling all the numeric columns -> (sample - mean) / std
num_cols = ['#user_prompts', '#error', '#no', '#thank', '#next', '#entropy',
       'prompt_avg_chars', 'response_avg_chars', 'total_prompts',
       'error_ratio', 'avg_entropy_per_prompt', 'total_chars_per_interaction',
       'prompt_response_char_ratio', 'pos_to_neg_ratio', 'response_complexity',
       'prompt_to_error_ratio', 'thank_you_frequency',
       'average_response_entropy', 'response_length',
       'response_sentiment_score', 'repeating_prompt_frequency',
       'whichQuestStartAskGPT', 'primitive_grade', 'ner_count']
scaler = MinMaxScaler()
scaled_df = df.copy()
scaled_df[num_cols] = scaler.fit_transform(scaled_df[num_cols])
print('The data has been scaled successfully!')

scaled_df

"""A brief explanalation for this piece of code:

1) A left merge is performed between df and scores DataFrames based on the "code" column.

2) Drop rows with NaN values from the merged DataFrame.

3) Drop duplicate rows based on the "code" column, keeping only the first occurrence of each code.
"""

selected_features = ['total_chars_per_interaction', 'repeating_prompt_frequency', 'whichQuestStartAskGPT', 'primitive_grade'] #selected from corr heatmap

# Select relevant data
selected_data = df[selected_features]

X = selected_data.to_numpy()
y = df["grade"].to_numpy()
print(X.shape, y.shape)

"""In this piece of code, for machine learning, the feature matrix "X" and target array "y" is prepared. Target array is "grade".

# Train/Test Data
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("Train set size:", len(X_train))
print("Test set size:", len(X_test))

"""In this piece of code, the data is splitted into training and test sets, so we can train our model on one portion (80%) of the data and evaluate its performance on another independent portion (20%).

### 1.1 **Decision Tree Regressor**

This model is run for analysis on dataframe.
"""

from sklearn.model_selection import KFold
# Crating an array of different values of max_depth
max_depth_arr = np.arange(1, 50)

# Now we want to evaluate the decision tree model on all these values
# We loop through all max_depth values and conduct a cross validation
# evaluation for each max_depth value

# Arrays to save the average cross-validation scores for each max_depth value
# on the train and validation data of each split
train_score_arr = []
val_score_arr = []
for max_depth in max_depth_arr:

  # Conducting cross validation
  skf = KFold(n_splits=5)

  # Arrays to save accuracies for each fold split
  fold_train_score_arr = []
  fold_val_score_arr = []
  for i, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):
    xt = X_train[train_idx]
    yt = y_train[train_idx]

    xv = X_train[val_idx]
    yv = y_train[val_idx]

    model = DecisionTreeRegressor(
        criterion='squared_error',
        random_state=42,
        max_depth=max_depth)

    # Fitting the model
    model.fit(xt, yt)

    # Getting predictions
    y_pred_train = model.predict(xt)
    y_pred_val = model.predict(xv)

    # Computing accuracy

    # Train
    error = mean_squared_error(y_pred_train, yt)
    fold_train_score_arr.append(error)

    # Validation
    error = mean_squared_error(y_pred_val, yv)
    fold_val_score_arr.append(error)

  # After running all splits, we compute the avgof accuracies in
  # the cross-validation run
  train_score_mean = np.mean(fold_train_score_arr)
  val_score_mean = np.mean(fold_val_score_arr)

  # Appending the avg/std scores of this max_depth value
  train_score_arr.append(train_score_mean)
  val_score_arr.append(val_score_mean)

fig, ax = plt.subplots(figsize=(12, 4))

ax.plot(max_depth_arr, train_score_arr, color='blue', alpha=0.7, label='Train scores')
ax.plot(max_depth_arr, val_score_arr, color='orange', alpha=0.7, linestyle='--', label='Validation scores')
plt.legend()
ax.set_xlabel('max_depth')
ax.set_ylabel('Avg CV RMSE')

plt.show()

min_samples_split_arr = np.arange(5, 50)

# Now we want to evaluate the decision tree model on all these values
# We loop through all min_samples_split values and conduct a cross validation
# evaluation for each min_samples_split value

# Arrays to save the average cross-validation scores for each min_samples_split value
# on the train and validation data of each split
train_score_arr = []
val_score_arr = []
for min_samples_split in min_samples_split_arr:

  # Conducting cross validation
  skf = KFold(n_splits=5)

  # Arrays to save accuracies for each fold split
  fold_train_score_arr = []
  fold_val_score_arr = []
  for i, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):
    xt = X_train[train_idx]
    yt = y_train[train_idx]

    xv = X_train[val_idx]
    yv = y_train[val_idx]

    model = DecisionTreeRegressor(
        criterion='squared_error',
        random_state=42,
        max_depth=8,
        min_samples_split = min_samples_split)

    # Fitting the model
    model.fit(xt, yt)

    # Getting predictions
    y_pred_train = model.predict(xt)
    y_pred_val = model.predict(xv)

    # Computing accuracy

    # Train
    error = mean_squared_error(y_pred_train, yt, squared=False)
    fold_train_score_arr.append(error)

    # Validation
    error = mean_squared_error(y_pred_val, yv, squared=False)
    fold_val_score_arr.append(error)

  # After running all splits, we compute the avgof accuracies in
  # the cross-validation run
  train_score_mean = np.mean(fold_train_score_arr)
  val_score_mean = np.mean(fold_val_score_arr)

  # Appending the avg/std scores of this max_depth value
  train_score_arr.append(train_score_mean)
  val_score_arr.append(val_score_mean)

fig, ax = plt.subplots(figsize=(12, 4))

ax.plot(min_samples_split_arr, train_score_arr, color='blue', alpha=0.7, label='Train scores')
ax.plot(min_samples_split_arr, val_score_arr, color='orange', alpha=0.7, linestyle='--', label='Validation scores')
plt.legend()
ax.set_xlabel('min_samples_split')
ax.set_ylabel('Avg CV RMSE')

plt.show()

regressor = DecisionTreeRegressor(random_state=0, criterion='squared_error', max_depth=8, min_samples_split = 20 )
regressor.fit(X_train, y_train)

extracted_MSEs = regressor.tree_.impurity
for idx, MSE in enumerate(regressor.tree_.impurity):
    print("Node {} has MSE {}".format(idx,MSE))

# Plotting the Tree
dot_data = tree.export_graphviz(regressor, out_file=None, feature_names=df.columns[1:-1])
graph = graphviz.Source(dot_data)
graph.render("hw")

"""The given pdf has the corresponding decision tree plot."""

# Prediction
y_train_pred = regressor.predict(X_train)
y_test_pred = regressor.predict(X_test)

# Calculation of Mean Squared Error (MSE)
print("MSE Train:", mean_squared_error(y_train,y_train_pred))
print("MSE TEST:", mean_squared_error(y_test,y_test_pred))

# Calculation of R-Squared (R2)
print("R2 Train:", r2_score(y_train,y_train_pred))
print("R2 TEST:", r2_score(y_test,y_test_pred))

y_test_pred

"""MSE (Mean Squared Error):

MSE for the training set is 1.83, which means the squared difference between the predicted and actual values is relatively low. The model fits the training data well. But, more importantly, model should fit the test data well.

MSE for the test set is 164.80, which is much more higher than the training MSE. Thus, there might be an overfitting issue, as the model cannot seem to generalize well to unseen data.

R2 (R-squared):

R2 for the training set is 0.99, which means that the model explains approximately 99% of the variance in grades. It is quite high, but again, the performance on the test set is more important.

R2 for the test set is -0.47, which is a negative value and may imply that the model is performing even worse than a naive baseline model. R2 = -0.47 indicates that the model can be overfitting and is not actually capturing the variance in the grades.

### 1.2 **Neural Network Model**

Since we have a numerical dataset, we chose using a feedforward neural network (multilayer perceptron). Five features are selected with highest correlation to grade column, extracted from correlation heatmap. This map is shown in data exploration part.
"""

# Define target column and selected features
target_column = 'grade'
selected_features = ['total_chars_per_interaction', 'repeating_prompt_frequency', 'response_avg_chars', 'prompt_avg_chars', '#user_prompts']

# Select relevant data
selected_data = df[selected_features + [target_column]]

# Split the data into training and testing sets
train_data, test_data = train_test_split(selected_data, test_size=0.2, random_state=42)

# Print the shapes of train and test data
print("Train data shape:", train_data[selected_features].shape)
print("Test data shape:", test_data[selected_features].shape)

# Scale the data
scaler = StandardScaler()
train_data[selected_features] = scaler.fit_transform(train_data[selected_features])
test_data[selected_features] = scaler.transform(test_data[selected_features])

# Define the neural network model
model = keras.Sequential([
    layers.Dense(256, activation='relu', input_shape=(len(selected_features),)),
    layers.Dropout(0.5),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1, activation='linear')
])

# Compile the model using the custom accuracy metric
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

# Display the model summary
model.summary()

# Prepare labels for training and testing
train_labels = np.array(train_data[target_column])
test_labels = np.array(test_data[target_column])

# Set up early stopping callback to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train the model
history = model.fit(
    train_data[selected_features], train_labels,
    epochs=22,  # You can adjust the number of epochs
    batch_size=4,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=1
)

"""Architecture:

Input Layer:

Neurons: Equal to the number of selected features (len(selected_features)), which is 5 in this case.
Hidden Layer 1:

Neurons: 256 neurons.
Activation Function: Rectified Linear Unit (ReLU).
Dropout Layer 1:

Dropout Rate: 0.5.
Hidden Layer 2:

Neurons: 128 neurons.
Activation Function: Rectified Linear Unit (ReLU).
Dropout Layer 2:

Dropout Rate: 0.5.
Output Layer:

Neurons: Single neuron (regression task).
Activation Function: Linear.
Compilation:

Optimizer:

Adam optimizer is used for gradient-based optimization.
Loss Function:

Mean Squared Error (MSE) is chosen as the loss function, suitable for regression tasks.
Metrics:

Mean Absolute Error (MAE) is chosen as an additional metric to monitor during training.
Training:

The model is trained for 18 epochs.

Batch size: 4.

Validation data (20% of training data) is used for monitoring.

Early stopping is applied with a patience of 3 epochs to prevent overfitting.

The model's training progress is stored in the history variable.

Evaluation of NN Model
"""

# Prediction on the training set
y_train_pred = regressor.predict(X_train)

# Prediction on the test set
y_test_pred = regressor.predict(X_test)

# Calculation of Mean Squared Error (MSE) for the training set
print("MSE Train:", mean_squared_error(y_train, y_train_pred))

# Calculation of Mean Squared Error (MSE) for the test set
print("MSE Test:", mean_squared_error(y_test, y_test_pred))

# Calculation of R-Squared (R2) for the training set
print("R2 Train:", r2_score(y_train, y_train_pred))

# Calculation of R-Squared (R2) for the test set
print("R2 Test:", r2_score(y_test, y_test_pred))

"""MSE (Mean Squared Error):

MSE for the training set is 1.83, which is a relatively lower difference between the actual values and the predicted values. The model fits the training data well.

MSE for the test set is 164.8, which is a relatively high value implying that the model's predictions deviate more from the actual values. The model has a poorer performance on unseen data.

R2 (R-Squared):

R2 for the training set is 0.99, which is very close to 1, implies a better fit to the training data.

R2 for the test set is -0.47, which is a negative number, implies that the model performs poorly on the unseen data, and the predictions are even worse than the simple mean.

There is a possible overfitting problem with the model. In order to improve the model's performance on the test set, techniques such as cross-validation, or hyperparamater tuning can be applied. Yet, the major problem with our models is that the features are not that correlated or explanatory with the "grade".

### 1.3 **Random Forest Algorithm**
"""

from sklearn.ensemble import RandomForestRegressor

random_forest_model = RandomForestRegressor(n_estimators=1000, max_depth=10, random_state=42)
random_forest_model.fit(X_train, y_train)

# Prediction
y_train_pred_rf = random_forest_model.predict(X_train)
y_test_pred_rf = random_forest_model.predict(X_test)

print(y_test_pred_rf)

"""Evaluation of The Model"""

# Calculation of Mean Squared Error (MSE) for the training set
print("MSE Train:", mean_squared_error(y_train,y_train_pred_rf))

# Calculation of Mean Squared Error (MSE) for the test set
print("MSE Test:", mean_squared_error(y_test,y_test_pred_rf))

# Calculation of R-Squared (R2) for the training set
print("R2 Train:", r2_score(y_train,y_train_pred_rf))

# Calculation of R-Squared (R2) for the test set
print("R2 Test:", r2_score(y_test,y_test_pred_rf))

"""### 1.4 **XGBoost (Extreme Gradient Boosting) Model**

"""

import xgboost as xgb
xgb_model = xgb.XGBRegressor(objective = "reg:squarederror", n_estimators = 100, learning_rate = 0.05, max_depth = 4, gamma = 4)

xgb_model.fit(X_train,y_train)

# Prediction
y_train_pred_xgb = xgb_model.predict(X_train)
y_test_pred_xgb = xgb_model.predict(X_test)

print(y_test_pred_xgb)

# Calculation of Mean Squared Error (MSE)
print("MSE Train:", mean_squared_error(y_train,y_train_pred_xgb))
print("MSE TEST:", mean_squared_error(y_test,y_test_pred_xgb))

print("R2 Train:", r2_score(y_train,y_train_pred_xgb))
print("R2 TEST:", r2_score(y_test,y_test_pred_xgb))

"""# Data Exploration

This part of the project is a general overview of the dataset with some statistics such as the number of instances, features, basic descriptive statistics, and some visualizations.
"""

# Overview of the dataset
print("Number of instances:", len(df))
print("Number of features:", len(df.columns) - 2)  # Exclude 'code' and 'grade'
print("Descriptive Statistics:")
print(df.describe())

"""Distribution of Grades:"""

plt.figure(figsize=(10, 6))
plt.title('Distribution of Grades')
plt.hist(df["grade"], bins=20, color='skyblue', edgecolor='black', alpha=0.7)
plt.xlabel('Grade')
plt.ylabel('Count')
plt.show()

"""Box Plots for Selected Features:"""

# Box plot for selected features
sns.set(style="whitegrid")
selected_features = ["#error", "#no", "#thank", "#next", "#entropy", "prompt_avg_chars", "response_avg_chars"]
plt.figure(figsize=(14, 8))
sns.boxplot(data=df[selected_features])
plt.title("Box Plot of Selected Features")
plt.ylabel("Value")
plt.show()

"""Plotting user prompts distribution:"""

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x="#user_prompts", bins=20, kde=True, color='skyblue')
plt.title("Distribution of User Prompts")
plt.xlabel("Number of User Prompts")
plt.ylabel("Frequency")
plt.show()

"""Plotting correlation heatmap:"""

plt.figure(figsize=(12, 10))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=.5)
plt.title("Correlation Heatmap")
plt.show()

top_features = correlation_matrix['grade'].abs().sort_values(ascending=False).head(6).index[1:]
print("Top 5 features with highest correlation:")
print(top_features)

"""Pairplot for a subset of features:"""

# Pairplot for a subset of features
subset_features = ["#user_prompts", "#error", "#thank", "prompt_avg_chars", "response_avg_chars", "grade"]
df_subset = df[subset_features].reset_index(drop=True)  # Reset index and drop it

# Create a scatterplot matrix
sns.pairplot(df_subset, hue="grade", palette="viridis", plot_kws={'alpha':0.5})
plt.suptitle("Pairplot of Selected Features", y=1.02)
plt.show()

# Histograms
df.hist(figsize=(14, 12), bins=20, color='skyblue', edgecolor='black')
plt.suptitle('Histograms of Features', y=1.02)
plt.show()

"""Scatter Plots

Explore relationships between specific features and grades.
"""

# Scatter Plot
plt.figure(figsize=(14, 8))
sns.scatterplot(x='response_avg_chars', y='grade', data=df, hue='grade', palette='viridis')
plt.title('Scatter Plot of Response Avg Chars with Grade')
plt.show()

# Calculate total_prompts as the sum of #user_prompts and other prompt-related columns
df['total_prompts'] = df['#user_prompts'] + df['#error'] + df['#no'] + df['#thank'] + df['#next'] + df['#entropy']

# Now, let's check the first few rows of the updated DataFrame
print(df.head())

# Box Plot
plt.figure(figsize=(16, 8))
sns.boxplot(x='grade', y='total_prompts', data=df, palette='pastel')
plt.title('Box Plot of Total Prompts by Grade')
plt.show()

# Bar Plot
average_features = df.groupby('grade').mean().reset_index()
plt.figure(figsize=(14, 8))
sns.barplot(x='grade', y='total_prompts', data=average_features, palette='rocket')
plt.title('Average Total Prompts by Grade')
plt.show()

# Swarm Plot
plt.figure(figsize=(16, 8))
sns.swarmplot(x='grade', y='response_avg_chars', data=df, palette='Set2')
plt.title('Swarm Plot of Response Avg Chars by Grade')
plt.show()

# KDE Plot
plt.figure(figsize=(14, 8))
sns.kdeplot(data=df, x='prompt_avg_chars', hue='grade', fill=True, common_norm=False, palette='husl')
plt.title('KDE Plot of Prompt Avg Chars by Grade')
plt.show()

# PairGrid
g = sns.PairGrid(df, hue='grade', palette='husl')
g.map_upper(sns.scatterplot)
g.map_diag(sns.histplot)
g.add_legend()
plt.suptitle('PairGrid of Features Colored by Grade', y=1.02)
plt.show()

from sklearn.ensemble import RandomForestRegressor

# Assuming you've already defined X and y
rf = RandomForestRegressor()
rf.fit(X_train, y_train)

# Extract feature importances
feature_importances = rf.feature_importances_

# Plot radar chart
fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))

# Ensure the number of features match the number of ticks and labels
num_features = len(df.columns[1:-1])
angles = np.linspace(0, 2 * np.pi, num_features, endpoint=False)
feature_importances = np.concatenate((feature_importances, [feature_importances[0]]))
angles = np.concatenate((angles, [angles[0]]))

# Pad feature_importances to match the length of angles
while len(feature_importances) < len(angles):
    feature_importances = np.concatenate((feature_importances, [0]))

ax.fill(angles, feature_importances, color='b', alpha=0.25)
ax.set_yticklabels([])
ax.set_xticks(angles[:-1])
ax.set_xticklabels(df.columns[1:-1], ha='center', fontsize=10)

plt.title('Feature Importance Radar Chart', size=15)
plt.show()

# Violin plot to show the distribution of numeric columns
plt.figure(figsize=(16, 10))
sns.violinplot(data=df.drop(columns='code'))
plt.title('Violin Plot of Numeric Columns')
plt.show()

# Pairplot for a quick overview of relationships between features
plt.figure(figsize=(18, 12))
sns.pairplot(df.drop(columns='code'))
plt.suptitle('Pairplot of Numeric Columns', y=1.02)
plt.show()

# Regression plot for a selected pair of features
plt.figure(figsize=(14, 10))
sns.regplot(x='response_avg_chars', y='grade', data=df)
plt.title('Regression Plot: Response Average Characters vs. Grade')
plt.show()

from mpl_toolkits.mplot3d import Axes3D

# 3D Scatter plot using response_avg_chars, prompt_avg_chars, and grade
fig = plt.figure(figsize=(12, 10))
ax = fig.add_subplot(111, projection='3d')

ax.scatter(df['response_avg_chars'], df['prompt_avg_chars'], df['grade'], c='blue', marker='o')

ax.set_xlabel('Response Avg Chars')
ax.set_ylabel('Prompt Avg Chars')
ax.set_zlabel('Grade')

plt.title('3D Scatter Plot')
plt.show()

"""These are some features that can be useful when predicting the grade of a student."""

import seaborn as sns

# Assuming df is your DataFrame
plt.figure(figsize=(15, 8))
sns.violinplot(x='grade', y='#user_prompts', data=df, inner='quartile')
plt.title('Distribution of User Prompts by Grade', size=15)
plt.show()

# Assuming df_subset is your DataFrame with features and correlations
questions = df_subset.columns[:-1]  # Exclude the 'grade' column

# Calculate the angles
angles = np.linspace(0, 2 * np.pi, len(questions), endpoint=False).tolist()
angles += angles[:1]

# Selecting data for a particular row, change row_index accordingly
row_index = 0
data = df_subset.iloc[row_index, :-1].values.tolist()
data += data[:1]

# Plotting Radar chart
fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))
ax.fill(angles, data, color='b', alpha=0.25)
ax.set_yticklabels([])
ax.set_xticks(angles[:-1])
ax.set_xticklabels(questions)

plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# selected_data is the subset of features used for training certain models
# normalized_selected_data = normalize_to_range(selected_data, min_value=0, max_value=100)


# Assuming 'repeating_prompt_frequency' is a categorical feature
categorical_feature = 'repeating_prompt_frequency'

# Scatter plots for continuous features
continuous_features = ['total_chars_per_interaction', 'whichQuestStartAskGPT', 'primitive_grade']

for feature in continuous_features:
    plt.figure(figsize=(10, 6))
    sns.scatterplot(data=selected_data, x=feature, y='grade')
    plt.title(f'Scatter plot of {feature} vs Grade')
    plt.xlabel(feature)
    plt.ylabel('Grade')
    plt.show()

# Box plot for the categorical feature
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x=categorical_feature, y='grade')
plt.title(f'Box plot of {categorical_feature} vs Grade')
plt.xlabel(categorical_feature)
plt.ylabel('Grade')
plt.show()

plt.figure(figsize=(12, 10))
correlation_matrix = selected_data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=.5)
plt.title("Correlation Heatmap")
plt.show()